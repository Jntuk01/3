import pandas as pd
import numpy as np
import math
from pprint import pprint
data = {
    'Outlook': ['Sunny', 'Sunny', 'Overcast', 'Rain', 'Rain', 'Rain', 'Overcast', 'Sunny', 'Sunny', 'Rain', 'Sunny', 'Overcast', 'Overcast', 'Rain'],
    'Temperature': ['Hot', 'Hot', 'Hot', 'Mild', 'Cool', 'Cool', 'Cool', 'Mild', 'Cool', 'Mild', 'Mild', 'Mild', 'Hot', 'Mild'],
    'Humidity': ['High', 'High', 'High', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'Normal', 'Normal', 'High', 'Normal', 'High'],
    'Wind': ['Weak', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Weak', 'Weak', 'Strong', 'Strong', 'Weak', 'Strong'],
    'PlayTennis': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No']
}

df = pd.DataFrame(data)

# Calculate entropy
def entropy(target_col):
    elements, counts = np.unique(target_col, return_counts=True)
    entropy_val = np.sum([(-counts[i]/np.sum(counts)) * math.log2(counts[i]/np.sum(counts)) for i in range(len(elements))])
    return entropy_val

# Calculate Information Gain
def info_gain(data, split_attribute_name, target_name="PlayTennis"):
    total_entropy = entropy(data[target_name])
    vals, counts = np.unique(data[split_attribute_name], return_counts=True)

    weighted_entropy = np.sum([(counts[i]/np.sum(counts)) * entropy(data.where(data[split_attribute_name] == vals[i]).dropna()[target_name])
                               for i in range(len(vals))])

    information_gain = total_entropy - weighted_entropy
    return information_gain

# ID3 Algorithm
def id3(data, original_data, features, target_attribute_name="PlayTennis", parent_node_class=None):
    # If all target values have the same value, return it
    if len(np.unique(data[target_attribute_name])) <= 1:
        return np.unique(data[target_attribute_name])[0]

    # If dataset is empty, return the mode of the original dataset
    elif len(data) == 0:
        return np.unique(original_data[target_attribute_name])[
            np.argmax(np.unique(original_data[target_attribute_name], return_counts=True)[1])]

    # If features list is empty, return the mode target feature value in the current dataset
    elif len(features) == 0:
        return parent_node_class

    else:
        # Set the default value for the node
        parent_node_class = np.unique(data[target_attribute_name])[
            np.argmax(np.unique(data[target_attribute_name], return_counts=True)[1])]

        # Select feature with highest information gain
        item_values = [info_gain(data, feature, target_attribute_name) for feature in features]
        best_feature_index = np.argmax(item_values)
        best_feature = features[best_feature_index]

        tree = {best_feature: {}}

        # Grow the tree branch by branch
        for value in np.unique(data[best_feature]):
            sub_data = data.where(data[best_feature] == value).dropna()
            subtree = id3(sub_data, data, [i for i in features if i != best_feature], target_attribute_name, parent_node_class)
            tree[best_feature][value] = subtree

        return tree

# Build tree
features = df.columns[:-1].tolist()
decision_tree = id3(df, df, features)
print("Decision Tree:")
pprint(decision_tree)

# Classify a new sample
def predict(query, tree, default='Yes'):
    for key in list(query.keys()):
        if key in tree:
            try:
                result = tree[key][query[key]]
            except:
                return default

            if isinstance(result, dict):
                return predict(query, result)
            else:
                return result
    return default

# Test sample
sample = {
    'Outlook': 'rain',
    'Temperature': 'mild',
    'Humidity': 'High',
    'Wind': 'strong'
}

print("\nNew Sample:", sample)
prediction = predict(sample, decision_tree)
print("Prediction:", prediction)
